<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Andong Deng's Homepage</title>
  
  <meta name="author" content="Andong Deng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>邓安东 Andong Deng</name>
              </p>
              <p>I am a second year PhD student at <a href="https://www.crcv.ucf.edu/">Center for Research in Computer Vision</a>, <a href="https://www.ucf.edu/">University of Central Florida</a>, advised by <a href="https://www.crcv.ucf.edu/chenchen/">Dr. Chen Chen</a>.
              </p>
              <p>
                From 2021 to 2022, I have been spending a wonderful period of time at <a href="">GeWu Lab</a>, working with <a href="https://dtaoo.github.io/">Dr. Di Hu </a>, who is really a passionate and respectful young scholar and I learned a lot from him. I have also worked as a deep learning research intern at Big Data Lab, Baidu, working with <a href="https://scholar.google.com/citations?user=f9V0NZkAAAAJ&hl=en"> Dr. Xingjian Li</a> and <a href="https://ix.cs.uoregon.edu/~dou/">Dr. Dejing Dou</a>. Prior to that, I obtained my Master degree from <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a> in 2021, and worked as a research assistant advised by <a href="https://me.sjtu.edu.cn/teacher_directory1/caiweiwei.html">Dr. Weiwei Cai</a>. I spent stupid years from 2016 to 2018, during which I obtained my bachelor degree from <a href="https://en.scu.edu.cn/">Sichuan University</a> in 2017.
              </p>
              <p>Beyond research, I regularly spend times on photography, music and a variety of sports (except soccer).</p>
              <p style="text-align:center">
                <a href="mailto:dengandong@knights.ucf.edu">Email</a> &nbsp/&nbsp
                <a href="data/AndongDeng_CV.pdf">Curriculum Vitae</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=FlP80_wAAAAJ">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/pink.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/pink.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Interests</heading>
              <p>
                I'm interested in computer vision, video understanding, multimodal learning, and cognitive science.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <p><sup>*</sup> equal contribution</p>
          </td>
        </tr>
      </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Pre-prints</heading>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Updates</heading>
            <p>
              [2023-07-14] Two papers accepted by ICCV 2023! 
            </p>
            <p>
              [2023-07-02] One paper accepted by IEEE TCSVT 2023! Congrats to Shoubin and Zhongying!
            </p>
            <p>
              [2023-03-13] One paper accepted by ICME 2023! Congrats to Wenke!
            </p>
            <p>
              [2022-03-09] I will be joining UCF this summer as a CS PhD student with Dr. Chen Chen.
            </p>
            <p>
              [2022-02-03] One paper accepted by CVPR 2022! Congrats to Xiaokang and Yake!
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/BEAR_teaser.jpg" alt="inadequate" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>A Large-scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition</papertitle>
              <br>
              <strong>Andong Deng<sup>*</sup></strong>, Taojiannan Yang<sup>*</sup>, and Chen Chen
              <br>
              <em>arXiv pre-print</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2303.13505.pdf">arxiv</a>  / <a href="https://github.com/AndongDeng/BEAR">code</a>
              <p>We introduce BEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18 video datasets grouped into 5 categories, which covers a diverse set of real-world applications.</p>
            </td>
          </tr> -->


          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/inadequate_pretrained.png" alt="inadequate" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Inadequately Pre-trained Models Are Better Feature Extractor</papertitle>
              <br>
              <strong>Andong Deng<sup>*</sup></strong>, Xingjian Li<sup>*</sup>, Zhibing Li, Di Hu, Chengzhong Xu and Dejing Dou
              <br>
              <em>arXiv pre-print</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2203.04668.pdf">arxiv</a>
              <p>Inadequately pre-trained models often extract better features than fully pre-trained ones. And deep models tend to first learn spectral components corresponding to large singular values.</p>
            </td>
          </tr> -->

          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/moprl_net.png" alt="moprl" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Regularity Learning via Explicit Distribution Modeling for Skeletal Video Anomaly Detection</papertitle>
              <br>
              Shoubin Yu, Zhongying Zhao, Haoshu Fang, <strong>Andong Deng</strong>, Haisheng Su, Dongliang Wang, Weihao Gan, Cewu Lu and Wei Wu
              <br>
              <em>arXiv pre-print</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2112.03649.pdf">arxiv</a>
              <p>A motion prior distribution is utilized to construct the pose representation. MoPRL achieves the state-of-the-art performance by an average improvement of 4.7% AUC on several challenging datasets.</p>
            </td>
          </tr> -->



        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/BEAR_teaser.jpg" alt="inadequate" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>A Large-scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition</papertitle>
              <br>
              <strong>Andong Deng<sup>*</sup></strong>, Taojiannan Yang<sup>*</sup>, and Chen Chen
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2303.13505.pdf">arxiv</a>  / <a href="https://github.com/AndongDeng/BEAR">code</a>
              <p>We introduce BEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18 video datasets grouped into 5 categories, which covers a diverse set of real-world applications.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/inadequate_pretrained.png" alt="inadequate" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Towards Inadequately Pre-trained Models in Transfer Learning</papertitle>
              <br>
              <strong>Andong Deng<sup>*</sup></strong>, Xingjian Li<sup>*</sup>, Di Hu, Tianyang Wang, Haoyi Xiong and Chengzhong Xu
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2203.04668.pdf">arxiv</a>
              <p>Inadequately pre-trained models often extract better features than fully pre-trained ones. And deep models tend to first learn spectral components corresponding to large singular values.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/moprl_net.png" alt="moprl" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Regularity Learning via Explicit Distribution Modeling for Skeletal Video Anomaly Detection</papertitle>
              <br>
              Shoubin Yu, Zhongying Zhao, Haoshu Fang, <strong>Andong Deng</strong>, Haisheng Su, Dongliang Wang, Weihao Gan, Cewu Lu and Wei Wu
              <br>
              <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2112.03649.pdf">arxiv</a>
              <p>A motion prior distribution is utilized to construct the pose representation. MoPRL achieves the state-of-the-art performance by an average improvement of 4.7% AUC on several challenging datasets.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/wenke_icme.png" alt="wenke_icme" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Robust Cross-Modal Knowledge Distillation for Unconstrained Videos</papertitle>
              <br>
              Wenke Xia, Xingjian Li, <strong>Andong Deng</strong>, Haoyi Xiong, Dejing Dou, and Di Hu
              <br>
              <em>ICME</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2304.07775.pdf">arxiv</a>
              <p>We propose a Modality Noise Filter module to erase the irrelevant noise in teacher modality with cross-modal context and design a Contrastive Semantic Calibration module to adaptively distill useful knowledge for target modality, by referring to the differentiated sample-wise semantic correlation in a contrastive fashion.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ogmge.png" alt="ogm-ge" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Balanced Multimodal Learning via On-the-fly Gradient Modulation</papertitle>
              <br>
              Xiaokang Peng<sup>*</sup>, Yake Wei<sup>*</sup>, <strong>Andong Deng</strong>, Dong Wang and Di Hu
              <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arxiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code</a>
              <p>Modulate gradients of two modalities to adaptively balance the optimization process of multimodal learning.</p>
            </td>
          </tr>


        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
              <p>
                Reviewer of CVPR 2022, ECCV 2022 and ICCV 2023
              </p>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:left;font-size:small;">
                Updated: July 16th, 2023.
              </p>
            </td>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thanks <a href="https://jonbarron.info/">Jon Barron</a> for this amazing template.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
